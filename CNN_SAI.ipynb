{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: keras in c:\\programdata\\anaconda3\\lib\\site-packages (2.4.3)Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pyyaml in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: scipy>=0.14 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.5.0)\n",
      "Requirement already satisfied: h5py in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from keras) (1.18.5)\n",
      "Requirement already satisfied: six in c:\\programdata\\anaconda3\\lib\\site-packages (from h5py->keras) (1.15.0)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in c:\\programdata\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: protobuf>=3.9.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.11.2)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.0)\n",
      "Requirement already satisfied: gast==0.3.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.3.3)\n",
      "Requirement already satisfied: numpy<1.19.0,>=1.16.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.18.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (3.3.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.1.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.33.2)\n",
      "Requirement already satisfied: tensorboard<3,>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: astunparse==1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.3.0)\n",
      "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (2.10.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.11.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorflow) (0.34.2)\n",
      "Requirement already satisfied: setuptools in c:\\programdata\\anaconda3\\lib\\site-packages (from protobuf>=3.9.2->tensorflow) (49.2.0.post20200714)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.0.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (2.24.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.7.0)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (0.4.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from tensorboard<3,>=2.3.0->tensorflow) (1.23.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (2020.6.20)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow) (1.25.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (1.3.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.1.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.5\" in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (4.6)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow) (3.1.0)\n",
      "Requirement already satisfied: pyasn1>=0.1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from rsa<5,>=3.1.4; python_version >= \"3.5\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow) (0.4.8)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "# %matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import os.path\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn import preprocessing\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Reshape, GRU\n",
    "from keras.layers import Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, GlobalAveragePooling1D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "training_data: (280, 797)\n",
      "test_data (69, 797)\n",
      "0 done\n",
      "training_data: (2933, 797)\n",
      "test_data (731, 797)\n",
      "1 done\n",
      "training_data: (5387, 797)\n",
      "test_data (1344, 797)\n",
      "2 done\n",
      "training_data: (7705, 797)\n",
      "test_data (1923, 797)\n",
      "3 done\n",
      "training_data: (9984, 797)\n",
      "test_data (2492, 797)\n",
      "4 done\n",
      "training_data: (13285, 797)\n",
      "test_data (3316, 797)\n",
      "5 done\n",
      "training_data: (14575, 797)\n",
      "test_data (3637, 797)\n",
      "12 done\n",
      "training_data: (17022, 797)\n",
      "test_data (4248, 797)\n",
      "13 done\n",
      "training_data: (19363, 797)\n",
      "test_data (4832, 797)\n",
      "14 done\n",
      "training_data: (20048, 797)\n",
      "test_data (5002, 797)\n",
      "15 done\n",
      "training_data: (22045, 797)\n",
      "test_data (5500, 797)\n",
      "23 done\n",
      "training_data: (24070, 797)\n",
      "test_data (6005, 797)\n",
      "24 done\n",
      "training_data: (25755, 797)\n",
      "test_data (6425, 797)\n",
      "25 done\n",
      "training_data: (27073, 797)\n",
      "test_data (6754, 797)\n",
      "34 done\n",
      "training_data: (28070, 797)\n",
      "test_data (7002, 797)\n",
      "35 done\n",
      "training_data: (28996, 797)\n",
      "test_data (7233, 797)\n",
      "45 done\n",
      "training_data: (31245, 797)\n",
      "test_data (7794, 797)\n",
      "123 done\n",
      "training_data: (33313, 797)\n",
      "test_data (8310, 797)\n",
      "124 done\n",
      "training_data: (35470, 797)\n",
      "test_data (8848, 797)\n",
      "125 done\n",
      "training_data: (37069, 797)\n",
      "test_data (9247, 797)\n",
      "134 done\n",
      "training_data: (39861, 797)\n",
      "test_data (9944, 797)\n",
      "135 done\n",
      "training_data: (41504, 797)\n",
      "test_data (10354, 797)\n",
      "145 done\n",
      "training_data: (42810, 797)\n",
      "test_data (10680, 797)\n",
      "234 done\n",
      "training_data: (44908, 797)\n",
      "test_data (11204, 797)\n",
      "235 done\n",
      "training_data: (46974, 797)\n",
      "test_data (11720, 797)\n",
      "245 done\n",
      "training_data: (48963, 797)\n",
      "test_data (12216, 797)\n",
      "345 done\n",
      "training_data: (51903, 797)\n",
      "test_data (12950, 797)\n",
      "1234 done\n",
      "training_data: (52579, 797)\n",
      "test_data (13118, 797)\n",
      "1235 done\n",
      "training_data: (54668, 797)\n",
      "test_data (13639, 797)\n",
      "1245 done\n",
      "training_data: (56426, 797)\n",
      "test_data (14077, 797)\n",
      "1345 done\n",
      "training_data: (57059, 797)\n",
      "test_data (14234, 797)\n",
      "2345 done\n",
      "training_data: (58802, 797)\n",
      "test_data (14669, 797)\n",
      "12345 done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2179, 32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 센서 1만 데이터 불러오기\n",
    "# h5 데이터는 이 파일과 같은 폴더에 있어야 함... 죄송함댜ㅎㅎ\n",
    "Label_size = 32\n",
    "sensor1_data = np.empty((0,765+Label_size),int)\n",
    "position_label = np.empty((0,Label_size),int)\n",
    "label_cnt = 0\n",
    "sensor1_data_training = np.empty((0,765+Label_size),int)\n",
    "sensor1_data_test = np.empty((0,765+Label_size),int)\n",
    "print(position_label)\n",
    "for ii in range(12346):\n",
    "#for ii in range(2):\n",
    "    filename ='20201022_Depth_{}.h5'.format(ii)\n",
    "    \n",
    "    if os.path.isfile(filename):\n",
    "        h5f = h5py.File(filename,'r')\n",
    "        a_dset_keys = list(h5f.keys())\n",
    "        ds_data = h5f[a_dset_keys[0]]\n",
    "        sensor1 = ds_data[:,0,:]    \n",
    "        \n",
    "        sensor1 = np.array(sensor1)\n",
    "        position_label = np.zeros((sensor1.shape[0],Label_size))\n",
    "        #print(position_label.shape)\n",
    "        #32bit coding\n",
    "        if(ii == 0):\n",
    "            aa = 1\n",
    "        else:\n",
    "            position_label[:,label_cnt] = 1\n",
    "        label_cnt += 1\n",
    "        \n",
    "        sensor1 = np.append(sensor1, position_label, axis=1)\n",
    "        \n",
    "        sensor_len = len(sensor1)\n",
    "        columns = range(0,sensor_len)\n",
    "        training_per = round(sensor_len*0.8)\n",
    "        test_per = sensor_len-training_per\n",
    "        training_data = sensor1[:training_per]\n",
    "        test_data = sensor1[training_per+1:]\n",
    "        \n",
    "        sensor1_data_training = np.append(sensor1_data_training, training_data, axis = 0)\n",
    "        sensor1_data_test = np.append(sensor1_data_test,test_data,axis = 0)\n",
    "        print(\"training_data:\", sensor1_data_training.shape)\n",
    "        print(\"test_data\",sensor1_data_test.shape)\n",
    "        \n",
    "        sensor1_data= np.append(sensor1_data,sensor1,axis = 0)\n",
    "        \n",
    "        #print(sensor1_data.shape)\n",
    "        #print(sensor1_data)\n",
    "        \n",
    "        \n",
    "        #sensor1_data_training = np.append(sensor1_data_training, sensor1[:training_per], axis = 0)\n",
    "        #sensor1_data_test= np.append(sensor1_data_test, sensor1[training_per+1:], axis = 0)\n",
    "        #sensor1_data_training = np.append(sensor1_data_training, sensor1_data_training_current,axis = 0)\n",
    "        #sensor1_data_test = np.append(sensor1_data_test,sensor1_data_test_current,axis = 0)\n",
    "        #print(\"Total data length:\",sensor1_data.shape)\n",
    "        #print(\"training data length:\",sensor1_data_training.shape)\n",
    "        #print(\"test data length:\",sensor1_data_test.shape)\n",
    "        \n",
    "        print(\"{} done\".format(ii))\n",
    "    else:\n",
    "        a = 1\n",
    "        \n",
    "\n",
    "position_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-1.1789647 ],\n",
       "        [-1.17934984],\n",
       "        [-1.18168469],\n",
       "        ...,\n",
       "        [ 0.57553962],\n",
       "        [ 0.57554661],\n",
       "        [ 0.57733117]],\n",
       "\n",
       "       [[-1.17466774],\n",
       "        [-1.17514132],\n",
       "        [-1.17756085],\n",
       "        ...,\n",
       "        [ 0.49354402],\n",
       "        [ 0.49355093],\n",
       "        [ 0.49531416]],\n",
       "\n",
       "       [[-1.19615253],\n",
       "        [-1.19618389],\n",
       "        [-1.19611813],\n",
       "        ...,\n",
       "        [ 0.74773039],\n",
       "        [ 0.74773755],\n",
       "        [ 0.7495669 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[ 0.63864834],\n",
       "        [ 0.64083185],\n",
       "        [ 0.6410522 ],\n",
       "        ...,\n",
       "        [-0.12962255],\n",
       "        [-0.12961626],\n",
       "        [-0.13621684]],\n",
       "\n",
       "       [[ 0.75466619],\n",
       "        [ 0.75446169],\n",
       "        [ 0.75445778],\n",
       "        ...,\n",
       "        [-0.23621683],\n",
       "        [-0.23621065],\n",
       "        [-0.24283895]],\n",
       "\n",
       "       [[ 0.61716355],\n",
       "        [ 0.61558078],\n",
       "        [ 0.61630917],\n",
       "        ...,\n",
       "        [-0.31001287],\n",
       "        [-0.31000677],\n",
       "        [-0.31665426]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc = preprocessing.StandardScaler()\n",
    "#Training Data, test Data => DataFrame화\n",
    "sensor_len_train = len(sensor1_data_training)\n",
    "columns_train = range(0,sensor_len_train)\n",
    "sensor_len_test = len(sensor1_data_test)\n",
    "columns_test = range(0,sensor_len_test)\n",
    "\n",
    "df_train = pd.DataFrame(data = sensor1_data_training, index = columns_train)\n",
    "df_test = pd.DataFrame(data = sensor1_data_test, index = columns_test)\n",
    "\n",
    "#Scaling\n",
    "#df_train[:,:764] = sc.fit_transform(df_train[:764])\n",
    "#df_test[:,:764] = sc.transform(df_test[:,:764])\n",
    "\n",
    "# train 데이터,Test 데이터 준비\n",
    "X_train = df_train.loc[:,:764]\n",
    "X_test = df_test.loc[:,:764]\n",
    "#y_train = df_train.loc[:,765:]\n",
    "#y_test = df_test.loc[:,765:]\n",
    "y_train = sensor1_data_training[:,765:]\n",
    "y_test = sensor1_data_test[:,765:]\n",
    "\n",
    "#Scaling\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "#CNN 모델 적용을 위한 reshape\n",
    "X_train = X_train.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "X_test = X_test.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "#X_train = X_train.values.reshape((X_train.shape[0],X_train.shape[1],1))\n",
    "#X_test = X_test.values.reshape((X_test.shape[0],X_test.shape[1],1))\n",
    "#y_train.shape\n",
    "#y_train\n",
    "y_test\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add_feature를 위한 함수 정의\n",
    "# 우리 데이터 형태에 맞게 수정-> 현재 Column 인덱스를 숫자 0~764로 정의\n",
    "def find_peak_global_rolling(df, thres, window):\n",
    "    df_peak = df[np.abs(df) > thres]\n",
    "    df_peak.columns = df.columns + '_peak'\n",
    "\n",
    "    df_count = df_peak.rolling(window, min_periods=1).count()\n",
    "    df_count.columns = df.columns + '_count'+ str(window)\n",
    "    return df_peak, df_count\n",
    "def find_peak_global(df, thres):\n",
    "    df_peak = df[np.abs(df) > thres]\n",
    "    df_peak.columns = df.columns + '_peak'\n",
    "    return df_peak\n",
    "def rolling_max(df, window):\n",
    "    df_max = df.rolling(window, min_periods=1).max()\n",
    "    #df_max.columns = [col + \"_max\" + str(window) for col in df.columns]\n",
    "    df_max.columns = [col for col in df.columns]\n",
    "    return df_max\n",
    "def rolling_std(df, window):\n",
    "    df_std = df.rolling(window, min_periods=1).std().fillna(0)\n",
    "    #df_std.columns = [col + \"_std\" + str(window) for col in df.columns]\n",
    "    df_std.columns = [col for col in df.columns]\n",
    "    return df_std\n",
    "def rolling_average(df, window):\n",
    "    df_av = df.rolling(window, min_periods=1).mean()\n",
    "    #df_av.columns = [col + \"_av\" + str(window) for col in df.columns]\n",
    "    df_av.columns = [col for col in df.columns]\n",
    "    return df_av"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d (Conv1D)              (None, 746, 64)           1344      \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 727, 64)           81984     \n",
      "_________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D) (None, 242, 64)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 223, 128)          163968    \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                4128      \n",
      "=================================================================\n",
      "Total params: 251,424\n",
      "Trainable params: 251,424\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#CNN모델\n",
    "model_m = Sequential()\n",
    "model_m.add(Conv1D(64, 20, activation='relu',input_shape=(765,1)))\n",
    "model_m.add(Conv1D(64, 20, activation='relu'))\n",
    "model_m.add(MaxPooling1D(3))\n",
    "model_m.add(Conv1D(128, 20, activation='relu'))\n",
    "model_m.add(GlobalAveragePooling1D())\n",
    "#model_m.add(MaxPooling1D(3))\n",
    "#model_m.add(Flatten())\n",
    "model_m.add(Dropout(0.2))\n",
    "model_m.add(Dense(32, activation='softmax'))\n",
    "print(model_m.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "941/941 [==============================] - 561s 596ms/step - loss: 0.6964 - accuracy: 0.7684 - val_loss: 29.8395 - val_accuracy: 0.1634\n",
      "Epoch 2/2\n",
      "268/941 [=======>......................] - ETA: 6:01 - loss: 0.0572 - accuracy: 0.9774"
     ]
    }
   ],
   "source": [
    "callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=5)\n",
    "]\n",
    "model_m.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "history = model_m.fit(X_train,\n",
    "                      y_train,\n",
    "                      batch_size=50,\n",
    "                      epochs=2,\n",
    "                      callbacks=callbacks_list,\n",
    "                      validation_split=0.2,\n",
    "                      verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_results(y_test, y_pred):\n",
    "    max_y_pred_test = np.argmax(y_pred, axis=1)\n",
    "    max_y_test = np.argmax(y_test, axis=1)\n",
    "    show_confusion_matrix(max_y_test, max_y_pred_test)\n",
    "    print(classification_report(max_y_test, max_y_pred_test))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(y_test, axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model_m.predict(X_test)\n",
    "show_results(y_test, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
